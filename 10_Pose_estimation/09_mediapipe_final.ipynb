{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16b49484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3257a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../data/models/pose_landmarker_heavy.task'\n",
    "VIDEO_INPUT_PATH = \"../datasets/videos/merged.mp4\"\n",
    "VIDEO_OUTPUT_PATH = \"../data/results/pose_output.mp4\"\n",
    "CSV_OUTPUT_PATH = \"../data/results/pose_metrics.csv\"\n",
    "MAX_GAP = 2\n",
    "POSE_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 7), (0, 4), (4, 5), (5, 6), (6, 8),\n",
    "    (9, 10), (11, 12), (11, 13), (13, 15), (15, 17), (15, 19), (15, 21),\n",
    "    (17, 19), (12, 14), (14, 16), (16, 18), (16, 20), (16, 22), (18, 20),\n",
    "    (11, 23), (12, 24), (23, 24), (23, 25), (24, 26), (25, 27), (26, 28),\n",
    "    (27, 29), (28, 30), (29, 31), (30, 32), (27, 31), (28, 32)\n",
    "]\n",
    "TORSO_JOINTS = [11, 12, 23, 24]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29043d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6a108",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'else' statement on line 119 (1847533718.py, line 121)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpose_data.append(frame_data)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'else' statement on line 119\n"
     ]
    }
   ],
   "source": [
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=MODEL_PATH),\n",
    "    running_mode=VisionRunningMode.VIDEO,\n",
    "    output_segmentation_masks=False,\n",
    "    min_pose_presence_confidence=0.4,\n",
    "    min_tracking_confidence=0.4\n",
    ")\n",
    "\n",
    "def draw_landmarks_from_landmarks(rgb_image, pose_landmarks):\n",
    "    annotated_image = rgb_image.copy()\n",
    "    h, w, _ = annotated_image.shape\n",
    "\n",
    "    pts = []\n",
    "    for lm in pose_landmarks:\n",
    "        x = int(lm.x * w)\n",
    "        y = int(lm.y * h)\n",
    "        pts.append((x, y))\n",
    "\n",
    "    for s, e in POSE_CONNECTIONS:\n",
    "        if s < len(pts) and e < len(pts):\n",
    "            cv2.line(annotated_image, pts[s], pts[e], (0, 255, 0), 2)\n",
    "\n",
    "    for lm in pose_landmarks:\n",
    "        if lm.visibility < 0.5:\n",
    "            continue\n",
    "        x = int(lm.x * w)\n",
    "        y = int(lm.y * h)\n",
    "        cv2.circle(annotated_image, (x, y), 5, (0, 0, 255), -1)\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "last_valid_landmarks = None\n",
    "missing_count = 0\n",
    "pose_used = None\n",
    "interpolated = False\n",
    "\n",
    "with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "    cap = cv2.VideoCapture(VIDEO_INPUT_PATH)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {VIDEO_INPUT_PATH}\")\n",
    "        exit()\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video properties: {frame_width}x{frame_height} @ {fps} FPS, {total_frames} frames\")\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(VIDEO_OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    pose_data = []\n",
    "    \n",
    "    frame_count = 0\n",
    "    \n",
    "    print(\"Processing video frames...\")\n",
    "    with tqdm(total=total_frames, desc=\"Processing\") as pbar:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_timestamp_ms = int((frame_count / fps) * 1000)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "            pose_landmarker_result = landmarker.detect_for_video(mp_image, frame_timestamp_ms)\n",
    "            \n",
    "            if pose_landmarker_result.pose_landmarks:\n",
    "                last_valid_landmarks = pose_landmarker_result.pose_landmarks[0]\n",
    "                missing_count = 0\n",
    "                pose_used = last_valid_landmarks\n",
    "                interpolated = False\n",
    "            else:\n",
    "                missing_count += 1\n",
    "                if last_valid_landmarks is not None and missing_count <= MAX_GAP:\n",
    "                    stable = all(\n",
    "                        lm.visibility > 0.7\n",
    "                        for i, lm in enumerate(last_valid_landmarks)\n",
    "                        if i in TORSO_JOINTS\n",
    "                    )\n",
    "\n",
    "                    if stable:\n",
    "                        pose_used = last_valid_landmarks\n",
    "                        interpolated = True\n",
    "                    else:\n",
    "                        pose_used = None\n",
    "                        interpolated = False\n",
    "                else:\n",
    "                    pose_used = None\n",
    "                    interpolated = False\n",
    "                    \n",
    "            if pose_used is not None:\n",
    "                annotated_image = draw_landmarks_from_landmarks(rgb_frame, pose_used)\n",
    "            else:\n",
    "                annotated_image = rgb_frame\n",
    "            annotated_bgr = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            out.write(annotated_bgr)\n",
    "            \n",
    "            frame_data = {\n",
    "                \"frame\": frame_count,\n",
    "                \"timestamp_ms\": frame_timestamp_ms,\n",
    "                \"model_pose_detected\": pose_landmarker_result.pose_landmarks is not None,\n",
    "                \"pose_used\": pose_used is not None,\n",
    "                \"interpolated\": interpolated,\n",
    "                \"missing_streak\": missing_count\n",
    "            }\n",
    "\n",
    "            if pose_used is not None:\n",
    "                for i, lm in enumerate(pose_used):\n",
    "                    frame_data[f\"lm_{i}_x\"] = lm.x\n",
    "                    frame_data[f\"lm_{i}_y\"] = lm.y\n",
    "                    frame_data[f\"lm_{i}_z\"] = lm.z\n",
    "                    frame_data[f\"lm_{i}_vis\"] = lm.visibility\n",
    "            else:\n",
    "                for i in range(33):\n",
    "                    frame_data[f\"lm_{i}_x\"] = None\n",
    "                    frame_data[f\"lm_{i}_y\"] = None\n",
    "                    frame_data[f\"lm_{i}_z\"] = None\n",
    "                    frame_data[f\"lm_{i}_vis\"] = None\n",
    "\n",
    "            pose_data.append(frame_data)\n",
    "            frame_count += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    if pose_data:\n",
    "        df = pd.DataFrame(pose_data)\n",
    "        df.to_csv(CSV_OUTPUT_PATH, index=False)\n",
    "        print(f\"\\nPose metrics saved to: {CSV_OUTPUT_PATH}\")\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "    else:\n",
    "        print(\"\\nNo pose landmarks detected in the video.\")\n",
    "    \n",
    "    print(f\"Annotated video saved to: {VIDEO_OUTPUT_PATH}\")\n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854fdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef2f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
