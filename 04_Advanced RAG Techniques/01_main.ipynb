{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347cb050",
   "metadata": {},
   "source": [
    "# Langchain documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa50e61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Manually typed', 'author': 'Sparsh Sahu', 'page': 1, 'date_created': datetime.datetime(2025, 11, 29, 14, 49, 13, 162951, tzinfo=datetime.timezone.utc)}, page_content='Hii this is the page content of a document')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from datetime import datetime, UTC\n",
    "\n",
    "doc = Document(\n",
    "    page_content='Hii this is the page content of a document',\n",
    "    metadata = {\n",
    "        \"source\" : \"Manually typed\",\n",
    "        \"author\" : \"Sparsh Sahu\",\n",
    "        \"page\" : 1,\n",
    "        \"date_created\" :  datetime.now(UTC),\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab31418",
   "metadata": {},
   "source": [
    "# Textfile to Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc21f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ecf95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('../data/text_files/sample.txt')\n",
    "doc2 = loader.load()\n",
    "# doc2\n",
    "# print(doc2[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "369ebf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 995.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source': '..\\\\data\\\\text_files\\\\sample.txt'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    path = \"../data/text_files\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding':'utf-8'},\n",
    "    show_progress=True,\n",
    ")\n",
    "doc3 = dir_loader.load()\n",
    "doc3[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9514c",
   "metadata": {},
   "source": [
    "# Splitting docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77ef0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aaa32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40a8739e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_documents(doc3)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843da52",
   "metadata": {},
   "source": [
    "# Embedding and VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "423f9464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.57.3\n",
      "Sentence Transformers version: 5.1.2\n",
      "Transformers path: e:\\ROBOTRONIX\\Learning Period\\venv\\Lib\\site-packages\\transformers\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import sentence_transformers\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Sentence Transformers version: {sentence_transformers.__version__}\")\n",
    "print(f\"Transformers path: {transformers.__file__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4afe78af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6b2f0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x14ac0e03da0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        \n",
    "    def generate_embeddings(self, text: List[str]):\n",
    "        embeddings = self.model.encode(text, show_progress_bar=True)\n",
    "        return embeddings\n",
    "    \n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9824036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x14ac031d250>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collection_name: str = 'pdf_docs', persist_directory: str = \"../data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "        \n",
    "    def _initialize_store(self):\n",
    "        os.makedirs(self.persist_directory, exist_ok=True)\n",
    "        self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "        \n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=self.collection_name,\n",
    "            metadata={\n",
    "                \"description\": \"Document embeddings for RAG Pipeline\",\n",
    "                \"hnsw:space\": \"cosine\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc,embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['context_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            documents_text.append(doc.page_content)\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "            \n",
    "        # Adding to db \n",
    "        self.collection.add(\n",
    "            ids=ids,\n",
    "            embeddings=embeddings_list,\n",
    "            metadatas=metadatas,\n",
    "            documents=documents_text\n",
    "        )\n",
    "        \n",
    "vector_store = VectorStore()\n",
    "vector_store\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a061bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "texts = [chunk.page_content for chunk in chunks]\n",
    "print(len(texts))\n",
    "\n",
    "embedding = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "vector_store.add_documents(chunks, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ba86f",
   "metadata": {},
   "source": [
    "# RAG Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28adae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 78.41it/s]\n"
     ]
    }
   ],
   "source": [
    "def retrieve(query: str, top_k: int = 5, score_threshold: float = 0.5):\n",
    "    query_embeddings = embedding_manager.generate_embeddings([query])\n",
    "    \n",
    "    # Search in vectorstore\n",
    "    results = vector_store.collection.query(\n",
    "        query_embeddings=query_embeddings,\n",
    "        n_results=top_k,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "results = retrieve(query='What is uv python package?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab4bcd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "780a7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "        \n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.4):\n",
    "        query_embeddings = self.embedding_manager.generate_embeddings([query])\n",
    "        \n",
    "        # Search in vectorstore\n",
    "        results = self.vector_store.collection.query(\n",
    "            query_embeddings=query_embeddings,\n",
    "            n_results=top_k,\n",
    "        )\n",
    "        retrieved_docs = []\n",
    "    \n",
    "        ids = results['ids'][0]\n",
    "        documents = results['documents'][0]\n",
    "        included = results['included'][0]\n",
    "        metadatas = results['metadatas'][0]\n",
    "        distances = results['distances'][0]\n",
    "        \n",
    "        for i, (doc_id, document, include, metadata, distance) in enumerate(zip(ids, documents, included, metadatas, distances)):\n",
    "            similarity_score = 1 - distance\n",
    "            print(similarity_score)\n",
    "            if similarity_score >= score_threshold:\n",
    "                retrieved_docs.append({\n",
    "                    'id': doc_id,\n",
    "                    'content': document,\n",
    "                    'included': include,\n",
    "                    'metadata': metadata,\n",
    "                    'similarity_score': similarity_score,\n",
    "                    'distance': distance,\n",
    "                    'rank': i + 1\n",
    "                })\n",
    "                \n",
    "        \n",
    "        return retrieved_docs\n",
    "    \n",
    "rag_retriever=RAGRetriever(vector_store,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df9b04df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 119.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5071104764938354\n",
      "0.419556200504303\n",
      "0.4033973217010498\n",
      "0.3981723189353943\n",
      "0.38645392656326294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_retriever.retrieve(query='What is Transformers and how to install it?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366caaf0",
   "metadata": {},
   "source": [
    "# Actual Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ec1cd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3679a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7660eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = 'openai/gpt-oss-20b', api_key: str = None):\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key\n",
    "        if not api_key:\n",
    "            print(\"No api key found\")\n",
    "            \n",
    "        self.llm = ChatGroq(\n",
    "            api_key=self.api_key,\n",
    "            model=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024,\n",
    "        )\n",
    "        \n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500):\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables = ['context', 'question'],\n",
    "            template=\"\"\"\n",
    "            You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "            Context: {context}.\n",
    "            Question: {question}.\n",
    "            Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        formatted_prompt = prompt.format(context=context, question=query)\n",
    "        \n",
    "        message = HumanMessage(content=formatted_prompt)\n",
    "        response = self.llm.invoke([message])\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def generate_response_simple(self, query: str, context: str):\n",
    "        prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "            Context: {context}.\n",
    "            Question: {query}.\n",
    "            Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        message = HumanMessage(content=prompt)\n",
    "        response = self.llm.invoke([message])\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbca99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = GroqLLM(api_key=os.environ['GROQ_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "241c4cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 110.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7350057363510132\n",
      "0.5937032699584961\n",
      "0.5174210667610168\n",
      "0.4836157560348511\n",
      "0.46603983640670776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context = rag_retriever.retrieve(\"What are Large Language Models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01d4abbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are advanced AI systems built on deep neural‑network architectures—most commonly the Transformer. They are trained on massive text corpora (books, articles, websites, etc.) and contain billions of parameters, which lets them learn patterns, grammar, and contextual meaning from language.  \n",
      "\n",
      "Key characteristics:\n",
      "\n",
      "| Feature | What it means |\n",
      "|---------|---------------|\n",
      "| **Transformer‑based** | Uses self‑attention to capture long‑range dependencies between words. |\n",
      "| **Massive training data** | Learns from diverse, large‑scale text collections. |\n",
      "| **Large parameter count** | Enables nuanced understanding and generation of text. |\n",
      "| **Fine‑tuning** | Can be adapted to specific tasks (e.g., translation, summarization, code generation). |\n",
      "| **Applications** | Chatbots, translation, content creation, question answering, debugging, documentation, etc. |\n",
      "| **Challenges** | Requires significant compute for training, can inherit biases from training data, needs careful monitoring. |\n",
      "\n",
      "Examples of popular LLMs include OpenAI’s ChatGPT (GPT‑4, GPT‑4o), Google Gemini, Anthropic Claude, Meta’s LLaMA, and open‑source models like Mistral and BLOOM. These models can generate human‑like text, answer questions, translate languages, and perform many other language‑related tasks.\n"
     ]
    }
   ],
   "source": [
    "response = groq_llm.generate_response_simple(query=\"What are Large Language Models?\", context=context)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cfbad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are advanced AI systems built on deep neural‑network architectures—most commonly the Transformer. They are trained on massive text corpora (books, articles, websites, etc.) and contain billions of parameters. This training lets them learn patterns, grammar, and contextual meaning, enabling them to:\n",
      "\n",
      "* **Understand** and generate human‑like text  \n",
      "* **Answer questions** and provide explanations  \n",
      "* **Translate languages** and summarize content  \n",
      "* **Assist with code generation, debugging, and documentation**  \n",
      "\n",
      "LLMs can be fine‑tuned for specific tasks or domains, making them versatile tools for chatbots, translation systems, content creation, and more. Examples include OpenAI’s ChatGPT, Google Gemini, Anthropic Claude, Meta’s LLaMA, and many open‑source variants.\n"
     ]
    }
   ],
   "source": [
    "response = groq_llm.generate_response(query=\"What are Large Language Models?\", context=context)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd3509",
   "metadata": {},
   "source": [
    "# Simple RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "103af3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    api_key=os.environ['GROQ_API_KEY'],\n",
    "    model='llama-3.1-8b-instant',\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    prompt = f\"\"\"\n",
    "        Use the following context to answer the question concisely.\n",
    "        Context: {context}\n",
    "        Question: {query}\n",
    "        Answer: \"\"\"\n",
    "    response = llm.invoke([prompt.format(context,query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad3a2c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5590577125549316\n",
      "0.5001290440559387\n",
      "0.47953134775161743\n",
      "An attention mechanism lets a model weigh different parts of an input sequence when computing each output element. In Transformers, self‑attention (or encoder‑decoder attention) calculates a weighted sum of value vectors, where the weights are derived from similarity scores between query and key vectors. This allows the model to focus on the most relevant tokens for each prediction, enabling richer contextual representations.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is attention mechanism?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780ae29",
   "metadata": {},
   "source": [
    "# Advance RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31623fb4",
   "metadata": {},
   "source": [
    "### 1. RAG With Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bc87379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    \n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    \n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7548ba29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 107.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6446174383163452\n",
      "0.4828724265098572\n",
      "0.46240782737731934\n",
      "Answer: The Masked Self-Attention Mechanism is a sub-layer in the Transformer architecture that prevents the model from attending to future tokens, maintaining the autoregressive property during generation.\n",
      "Sources: [{'source': '..\\\\data\\\\text_files\\\\sample.txt', 'page': 'unknown', 'score': 0.6446174383163452, 'preview': \"Masked Self-Attention Mechanism: Similar to the encoder's self-attention mechanism but its main purpose is to prevent attending to future tokens to maintain the autoregressive property (no cheating during generation).\\nEncoder-Decoder Attention Mechanism: This sub-layer allows the decoder to focus on...\"}, {'source': '..\\\\data\\\\text_files\\\\sample.txt', 'page': 'unknown', 'score': 0.4828724265098572, 'preview': 'Feed-Forward Network: The output from the self-attention mechanism is passed through a position-wise feed-forward network.\\nLayer Normalization and Residual Connections: Layer normalization and residual connections are applied.\\n3. Decoder Process\\nInput Embedding and Positional Encoding: The partially...'}, {'source': '..\\\\data\\\\text_files\\\\sample.txt', 'page': 'unknown', 'score': 0.46240782737731934, 'preview': '3. Transformer Architecture\\nSelf-Attention calculates how each word relates to others in the input. It uses Query (Q), Key (K) and Value (V) vectors.\\nMulti-head attention allows the model to focus on multiple relationships simultaneously.\\nA feedforward network processes attention outputs independent...'}]\n",
      "Confidence: 0.6446174383163452\n",
      "Context Preview: Masked Self-Attention Mechanism: Similar to the encoder's self-attention mechanism but its main purpose is to prevent attending to future tokens to maintain the autoregressive property (no cheating during generation).\n",
      "Encoder-Decoder Attention Mechanism: This sub-layer allows the decoder to focus on\n"
     ]
    }
   ],
   "source": [
    "result = rag_advanced('What is Masked self attention mechanism?', rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dffd751",
   "metadata": {},
   "source": [
    "### 2. Enhanced RAG Advanced Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "033c2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7bfb635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 114.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4813169240951538\n",
      "0.382609486579895\n",
      "0.37102580070495605\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "Improved ability to capture complex patterns in the data.\n",
      "Enhanced model capacity without significant increase in computational complexity.\n",
      "Mathematical Formulation:\n",
      "Given an input sequence X the self-attention mechanism computes three matrices: queri"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es Q, keys K and values V by multiplying X with learned weight matrices \n",
      "W\n",
      "Q\n",
      "W \n",
      "Q\n",
      "​\n",
      " ​, \n",
      "W\n",
      "K\n",
      "W \n",
      "K\n",
      "​\n",
      " ​ and \n",
      "W\n",
      "V\n",
      "W \n",
      "V\n",
      "​\n",
      " .\n",
      "\n",
      "Q\n",
      "=\n",
      "X\n",
      "W\n",
      "Q\n",
      ",\n",
      "K\n",
      "=\n",
      "X\n",
      "W\n",
      "K\n",
      ",\n",
      "V\n",
      "=\n",
      "X\n",
      "W\n",
      "V\n",
      "Q=XW \n",
      "Q\n",
      "​\n",
      " ,K=XW \n",
      "K\n",
      "​\n",
      " ,V=XW \n",
      "V\n",
      "​\n",
      " \n",
      "\n",
      "The attention scores are computed as:\n",
      "\n",
      "Attention\n",
      "(\n",
      "Q\n",
      ",\n",
      "K\n",
      ",\n",
      "V\n",
      ")\n",
      "=\n",
      "softmax\n",
      "(\n",
      "Q\n",
      "K\n",
      "T\n",
      "d\n",
      "k\n",
      ")\n",
      "Attention(Q,K,V)=softmax( \n",
      "d \n",
      "k\n",
      "​\n",
      " \n",
      "​\n",
      " \n",
      "QK \n",
      "T\n",
      " \n",
      "​\n",
      " )\n",
      "\n",
      "For multi-head attention, we apply self-attention multiple times:\n",
      "\n",
      "MultiHead\n",
      "(\n",
      "Q\n",
      ",\n",
      "K\n",
      ",\n",
      "V\n",
      ")\n",
      "=\n",
      "Concat\n",
      "(\n",
      "head\n",
      "1\n",
      ",\n",
      "…\n",
      ",\n",
      "head\n",
      "h\n",
      ")\n",
      "W\n",
      "O\n",
      "MultiHead(Q,K,V)=Concat(head \n",
      "1\n",
      "​\n",
      " ,…,head \n",
      "h\n",
      "​\n",
      " )W \n",
      "O\n",
      "​\n",
      " \n",
      "\n",
      "where Where each head is computed as:\n",
      "\n",
      "head\n",
      "i\n",
      "=\n",
      "Attention\n",
      "(\n",
      "Q\n",
      "W\n",
      "Q\n",
      "i\n",
      ",\n",
      "K\n",
      "W\n",
      "K\n",
      "i\n",
      ",\n",
      "V\n",
      "W\n",
      "V\n",
      "i\n",
      ")\n",
      "head \n",
      "i\n",
      "​\n",
      " =Attention(QW \n",
      "Q\n",
      "i\n",
      "​\n",
      " ,KW \n",
      "K\n",
      "i\n",
      "​\n",
      " ,VW \n",
      "V\n",
      "i\n",
      "​\n",
      " )\n",
      "\n",
      "Masked Self-Attention Mechanism: Similar to the encoder's self-attention mechanism but its main purpose is to prevent attending to future tokens to maintain the autoregressive property (no cheating during generation).\n",
      "Encoder-Decoder Attention Mechanism: This sub-layer allows the decoder to focus on relevant parts of the encoder's output representation. This allows the decoder to focus on relevant parts of the input, essential for tasks like translation.\n",
      "Feed-Forward Neural Network: This sub-layer processes the combined output of the masked self-attention and encoder-decoder attention mechanisms.\n",
      "In-Depth Analysis of Transformer Components\n",
      "1. Multi-Head Self-Attention Mechanism\n",
      "\n",
      "In-Depth Analysis of Transformer Components\n",
      "1. Multi-Head Self-Attention Mechanism\n",
      "Multi-head attention extends the self-attention mechanism by applying it multiple times in parallel with each \"head\" learning different aspects of the input data. This allows the model to capture a richer set of relationships within the input sequence. The outputs of these heads are then concatenated and linearly transformed to produce the final output. The benefits include:\n",
      "\n",
      "Question: what is attention is all you need\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: \"Attention is All You Need\" is a research paper published in 2017 by Vaswani et al. that introduced the Transformer model, a neural network architecture that relies entirely on self-attention mechanisms to process input sequences.\n",
      "\n",
      "Citations:\n",
      "[1] ..\\data\\text_files\\sample.txt (page unknown)\n",
      "[2] ..\\data\\text_files\\sample.txt (page unknown)\n",
      "[3] ..\\data\\text_files\\sample.txt (page unknown)\n",
      "Summary: The \"Attention is All You Need\" research paper, published in 2017 by Vaswani et al., introduced the Transformer model, a neural network architecture that uses self-attention mechanisms to process input sequences. This architecture relies solely on self-attention, eliminating the need for recurrent neural networks (RNNs) and convolutional neural networks (CNNs).\n",
      "History: {'question': 'what is attention is all you need', 'answer': '\"Attention is All You Need\" is a research paper published in 2017 by Vaswani et al. that introduced the Transformer model, a neural network architecture that relies entirely on self-attention mechanisms to process input sequences.', 'sources': [{'source': '..\\\\data\\\\text_files\\\\sample.txt', 'page': 'unknown', 'score': 0.4813169240951538, 'preview': 'Improved ability to capture complex patterns in the data.\\nEnhanced model capacity without significant increase in comput...'}, {'source': '..\\\\data\\\\text_files\\\\sample.txt', 'page': 'unknown', 'score': 0.382609486579895, 'preview': \"Masked Self-Attention Mechanism: Similar to the encoder's self-attention mechanism but its main purpose is to prevent at...\"}, {'source': '..\\\\data\\\\text_files\\\\sample.txt', 'page': 'unknown', 'score': 0.37102580070495605, 'preview': 'In-Depth Analysis of Transformer Components\\n1. Multi-Head Self-Attention Mechanism\\nMulti-head attention extends the self...'}], 'summary': 'The \"Attention is All You Need\" research paper, published in 2017 by Vaswani et al., introduced the Transformer model, a neural network architecture that uses self-attention mechanisms to process input sequences. This architecture relies solely on self-attention, eliminating the need for recurrent neural networks (RNNs) and convolutional neural networks (CNNs).'}\n"
     ]
    }
   ],
   "source": [
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        \n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is attention is all you need\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bcf2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
