{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797a9d72",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038bcf9",
   "metadata": {},
   "source": [
    "##### 1. Try a support vector machine regressor (sklearn.svm.SVR) with various hyperparameters, such as kernel=\"linear\" (with various values for the C hyperparameter) or kernel=\"rbf\" (with various values for the C and gamma hyperparameters). Note that support vector machines don’t scale well to large datasets, so you should probably train your model on just the first 5,000 instances of the training set and use only 3-fold cross-validation, or else it will take hours. How does the best SVR predictor perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0030db5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fad88a54",
   "metadata": {},
   "source": [
    "##### 2. Try replacing the GridSearchCV with a RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55df44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e5ae531",
   "metadata": {},
   "source": [
    "##### 3. Try adding a SelectFromModel transformer in the preparation pipeline to select only the most important attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb0fbf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fe9104a",
   "metadata": {},
   "source": [
    "##### 4. Try creating a custom transformer that trains a k-nearest neighbors regressor (sklearn.neighbors.KNeighborsRegressor) in its fit() method, and outputs the model’s predictions in its transform() method. Then add this feature to the preprocessing pipeline, using latitude and longitude as the inputs to this transformer. This will add a feature in the model that corresponds to the housing median price of the nearest districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33abe813",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cac5969c",
   "metadata": {},
   "source": [
    "##### 5. Automatically explore some preparation options using GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88cabe4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb7e9096",
   "metadata": {},
   "source": [
    "##### 6. Try to implement the StandardScalerClone class again from scratch, then add support for the inverse_transform() method: executing scaler.inverse_transform(scaler.fit_transform(X)) should return an array very close to X. Then add support for feature names: set feature_names_in_ in the fit() method if the input is a DataFrame. This attribute should be a NumPy array of column names. Lastly, implement the get_feature_names_out() method: it should have one optional input_features=None argument. If passed, the method should check that its length matches n_features_in_, and it should match feature_names_in_ if it is defined; then input_features should be returned. If input_features is None, then the method should either return feature_names_in_ if it is defined or np.array([\"x0\", \"x1\", ...]) with length n_features_in_ otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8c6ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
